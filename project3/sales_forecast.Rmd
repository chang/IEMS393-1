---
title: "Program Sales Forecast"
author: "Eric Chang"
date: "2/23/2017"
output: pdf_document
---

```{r, message=F, warning=F}
require(ggplot2)
source("featurize_data.R")
```

### Building initial regression model

#### Initial
```{r}
dat_lm <- filter(dat, !(programs_ordered == programs_sold)) %>% 
          select(-programs_ordered, -time_kickoff, -percent_male) %>% 
          filter(coach_pokey != 1) %>%  # take out old coach game
          select(-coach_pokey) %>% 
          filter(!(month == "August" & day_of_month == 30 & opponent == "Nebraska")) %>% 
          select(-opponent) %>% 
          mutate(year = as.integer(as.character(year)))

dat_lm$month <- droplevels(dat_lm$month)

dat_lm_fit <- lm(programs_sold ~ ., dat_lm)
message("R^2: ", summary(dat_lm_fit)$r.squared)
message("RMSE: ", sqrt(sum(dat_lm_fit$residuals ^ 2)/nrow(dat_lm)))
```

\newpage

### Outlier Detection
```{r}
cooks <- cooks.distance(dat_lm_fit)
plot(cooks)
dat_lm[cooks > 4 / nrow(dat_lm), ] %>% na.omit()  # extract outliers using Fox's threshold
```


#### Removing August/December
```{r}
dat_lm <- dat_lm %>% filter(!month %in% c("August", "December"))

message("Not cross validated, removed August and December points:")
dat_lm_fit <- lm(programs_sold ~ ., dat_lm)
message("R^2: ", summary(dat_lm_fit)$r.squared)
message("RMSE: ", sqrt(sum(dat_lm_fit$residuals ^ 2 / nrow(dat_lm) )))
```


### Cross validating

```{r}
lm_cross_validation <- function(formula, dat_lm, folds){
    # generate model matrix 
    set.seed(123)
    dat_lm$fold <- ceiling(runif(nrow(dat_lm)) * folds)
    dat_lm <- arrange(dat_lm, fold)
    dat_lm$oof_predictions <- NA
    
    summaries <- list()
        
    for (i in seq(1, folds)) {
        in_fold <- dat_lm[dat_lm$fold != i,]
        out_of_fold <- dat_lm[dat_lm$fold == i,]
        
        fit <- lm(formula, data=select(in_fold, -oof_predictions, -fold))
        summaries[[i]] <- summary(fit)
        
        dat_lm$oof_predictions[dat_lm$fold == i] <- 
          predict(fit, select(out_of_fold, -oof_predictions, -fold))
    }
    
    # calculate r-squared with out of fold predictions
    dat_lm$tss <- (dat_lm$programs_sold - mean(dat_lm$programs_sold))^2
    dat_lm$rss <- (dat_lm$programs_sold - dat_lm$oof_predictions)^2
    rsquared <- 1 - sum(dat_lm$rss)/sum(dat_lm$tss)
    
    message("R^2 CV: ", rsquared)
    message("RMSE CV: ", sqrt(sum(dat_lm$rss/nrow(dat_lm))))
    return(dat_lm)
}

message("Cross validated:")
lm_cross_validation("programs_sold ~ .", dat_lm, folds=nrow(dat_lm))

message("\n")

message("Cross validated with polynomial terms, removed outlier")
formula = "programs_sold ~ . + I(day_of_month^2) + month*day_of_month + I(year^2)"
cv_lm <- lm_cross_validation(formula, dat_lm, folds=nrow(dat_lm)-1)


```

```{r}
dat_fit <- lm(programs_sold ~ .+ I(day_of_month^2) + I(year^2) + day_of_month*month, dat_lm)

dat_fit$fitted.values
```



### Model Evaluation

```{r}
t <- lm(formula, dat_lm[c(-17, -21),])
summary(t)
```


```{r, eval=F}
require(xgboost)
dat_lm <- dat_lm[-17,]

# create model matricies taking out 17th observation
train_matrix <- Matrix::sparse.model.matrix(programs_sold ~ . + I(day_of_month^2) + month*day_of_month + I(year^2), dat_lm)
response <- dat_lm$programs_sold

eval_cv <- function(evaluation_log){
    # input: xgb_cross_validation$evaluation_log
    # output: min test rmse, number of iterations, and dataset size
    min_index <- which(evaluation_log$test_rmse_mean == min(evaluation_log$test_rmse_mean))
    message("Minimum test rmse: ", evaluation_log$test_rmse_mean[min_index], 
            "\nIterations: ", min_index,
            "\nDataset size: ", length(cv$folds[[1]]) * length(cv$folds))
    
    out = list("min_rmse" = evaluation_log$test_rmse_mean[min_index],
               "min_index" = min_index,
               "n" = length(cv$folds[[1]]) * length(cv$folds))
    return(out)
}

## TRAIN
cv <-
    xgb.cv(data=train_matrix,
           label=response,
           objective = "reg:linear",
           eta = .01,
           max_depth = 6,
           subsample = .8,
           colsample_bytree = .6,
           nfold = nrow(dat_lm),
           nrounds = 1500,
           early_stopping_rounds = 1500,
           print_every_n = 5,
           metrics="rmse")
eval_cv(cv$evaluation_log)

eval <- cv$evaluation_log %>% data.frame()
ggplot(eval) + 
  geom_line(aes(x=iter, y=train_rmse_mean), size=1, color='darkgray', alpha=1) + 
  geom_line(aes(x=iter, y=test_rmse_mean), size=1, color='blue', alpha=.6) +
  geom_point(data=eval[which(eval$test_rmse_mean == min(eval$test_rmse_mean)),], 
             aes(x=iter, y=test_rmse_mean), size=3, color='red', alpha=.6) + 
  ggtitle("XGBOOST cross validation") +
  ylab("RMSE") + xlab("Iteration #")

# 
# xgb_fit <-
#     xgboost(data=train_matrix,
#            label=response,
#            objective = "reg:linear",
#            eta = .05,
#            max_depth = 8,
#            subsample = .8,
#            nfold = 20,
#            nrounds = 125,
#            metrics="rmse")
# 
# xgb.importance(model = xgb_fit) %>% xgb.plot.importance()


```






